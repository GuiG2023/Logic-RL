import torch
from transformers import pipeline
import pandas as pd

print("ğŸš€ M2æ”¹è¿›ç‰ˆæµ‹è¯•")

# è®¾å¤‡æ£€æµ‹
device = "mps" if torch.backends.mps.is_available() else "cpu"
print(f"ğŸ–¥ï¸  è®¾å¤‡: {device}")

# åŠ è½½æ•°æ®
df = pd.read_parquet('./data/kk/instruct/3ppl/test.parquet')
question = str(df.iloc[0]['quiz'])  # ä½¿ç”¨quizåˆ—
print(f"â“ é—®é¢˜: {question[:100]}...")

# åŠ è½½æ¨¡å‹
generator = pipeline(
    "text-generation",
    model="Qwen/Qwen2.5-0.5B-Instruct",
    device=0 if device == "mps" else -1,
    torch_dtype=torch.float16 if device == "mps" else None
)

# ä½¿ç”¨æ›´å¼ºçš„æ ¼å¼è¦æ±‚
prompt = f"""<|im_start|>system
You MUST respond using this exact format:
<think>
Your reasoning process here
</think>
<answer>
Your final answer here
</answer>

IMPORTANT: Always include both <think></think> and <answer></answer> tags.
<|im_end|>
<|im_start|>user
{question}
<|im_end|>
<|im_start|>assistant
<think>"""

print("ğŸ§  å¼€å§‹æ¨ç†...")
result = generator(
    prompt, 
    max_new_tokens=256, 
    temperature=0.1,
    do_sample=False,  # ç¡®å®šæ€§ç”Ÿæˆ
    pad_token_id=generator.tokenizer.eos_token_id
)

response = result[0]['generated_text'][len(prompt):]
print(f"ğŸ’­ å›ç­”: {response}")

# æ£€æŸ¥æ ¼å¼
has_think = '</think>' in response
has_answer = '<answer>' in response and '</answer>' in response
format_correct = has_think and has_answer

print(f"\nğŸ“‹ æ ¼å¼æ£€æŸ¥:")
print(f"   Thinkæ ‡ç­¾: {'âœ…' if has_think else 'âŒ'}")
print(f"   Answeræ ‡ç­¾: {'âœ…' if has_answer else 'âŒ'}")
print(f"   æ ¼å¼æ­£ç¡®: {'âœ…' if format_correct else 'âŒ'}")

if format_correct:
    print("ğŸ‰ å®Œç¾ï¼M2æµ‹è¯•æˆåŠŸï¼Œæ ¼å¼æ­£ç¡®ï¼")
    print("âœˆï¸  å¯ä»¥è½¬åˆ°Colabåšå¤§è§„æ¨¡å®éªŒäº†")
else:
    print("ğŸ“ éœ€è¦è¿›ä¸€æ­¥è°ƒæ•´promptï¼Œä½†åŠŸèƒ½æ­£å¸¸")
    
    # å°è¯•è§£æç»“æœ
    if '</think>' in response:
        think_part = response.split('</think>')[0]
        print(f"ğŸ§  Thinkéƒ¨åˆ†: {think_part[:100]}...")
    
